{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "from torch.utils.data.sampler import RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = Path(\"../data\")\n",
    "DATA_DIR = Path(\"../data/mtl-dataset\")\n",
    "DATASETS = ['apparel', 'baby', 'books', 'camera_photo',  'electronics', \n",
    "      'health_personal_care', 'imdb', 'kitchen_housewares', 'magazines', \n",
    "      'music', 'software', 'sports_outdoors', 'toys_games', 'video']\n",
    "BSIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apparel.task.train'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = DATASETS[0]+\".task.train\"\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i ordered a pair for my husband and was very d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>love hush puppies . they are comfortable and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i was so excited to find these , they seemed t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  i ordered a pair for my husband and was very d...\n",
       "1      1  love hush puppies . they are comfortable and i...\n",
       "2      0  i was so excited to find these , they seemed t..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR / dataset_name, sep=\"\\t\", header=None, names=['label', 'text'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_list, data_dir, devset_fraction=0.2):\n",
    "    col_names=['label', 'text']\n",
    "    for idx, dataset in enumerate(dataset_list):\n",
    "        train_file = dataset+\".task.train\"\n",
    "        dev_file = dataset+\".task.dev\"\n",
    "        test_file = dataset+\".task.test\" \n",
    "        \n",
    "        df = pd.read_csv(data_dir / train_file, sep=\"\\t\", header=None, names=col_names)\n",
    "        train_df = df.sample(frac=1-devset_fraction).reset_index(drop=True)\n",
    "        dev_df = df.sample(frac=devset_fraction).reset_index(drop=True)\n",
    "        test_df = pd.read_csv(data_dir / test_file, sep=\"\\t\", header=None, names=col_names)\n",
    "        \n",
    "        train_df.to_csv(DATA_DIR / f\"{train_file}.csv\", index=False)\n",
    "        dev_df.to_csv(DATA_DIR / f\"{dev_file}.csv\", index=False)\n",
    "        test_df.to_csv(DATA_DIR / f\"{test_file}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data(dataset_list=DATASETS, data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = Field(sequential=True, tokenize=lambda x: x.split(), lower=True, batch_first=True, include_lengths=True)\n",
    "label_field = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [(\"label\", label_field), (\"text\", text_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets, dev_sets, test_sets = [], [], []\n",
    "for dataset in DATASETS:\n",
    "    train_file = dataset+\".task.train.csv\"\n",
    "    dev_file = dataset+\".task.dev.csv\"\n",
    "    test_file = dataset+\".task.test.csv\"\n",
    "    train_set, dev_set, test_set = TabularDataset.splits(path=DATA_DIR, root=DATA_DIR,\n",
    "                                                         train=train_file, validation=dev_file, test=test_file,\n",
    "                                                         fields=data_fields, skip_header=True, format=\"csv\")\n",
    "    train_sets.append(train_set)\n",
    "    dev_sets.append(dev_set)\n",
    "    test_sets.append(test_set)\n",
    "    \n",
    "text_field.build_vocab(*train_sets, vectors=GloVe(cache=GLOVE_DIR))\n",
    "train_sets = ConcatDataset(train_sets)\n",
    "dev_sets = ConcatDataset(dev_sets)\n",
    "test_sets = ConcatDataset(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 111264),\n",
       " ('.', 106635),\n",
       " (',', 85742),\n",
       " ('and', 58624),\n",
       " ('i', 54447),\n",
       " ('to', 53719),\n",
       " ('a', 52662),\n",
       " ('it', 44688),\n",
       " ('of', 41316),\n",
       " ('is', 36742)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'text'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_sets[0]\n",
    "example.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have purchased these beloved sport pants for years . the cut of these pants has been changed with this order . the quality of the cut and fabric is much lower now . i returned the item 0\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(example.text), example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchsampler for Multi-Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSchedulerSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    iterate over tasks and provide a random batch per task in each mini-batch\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.number_of_datasets = len(dataset.datasets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.number_of_datasets\n",
    "    \n",
    "    def __iter__(self):\n",
    "        samplers_list = []\n",
    "        sampler_iterators = []\n",
    "        datasets_length = []\n",
    "        for dataset_idx in range(self.number_of_datasets):\n",
    "            cur_dataset = self.dataset.datasets[dataset_idx]\n",
    "            sampler = RandomSampler(cur_dataset)\n",
    "            samplers_list.append(sampler)\n",
    "            cur_sampler_iterator = sampler.__iter__()\n",
    "            sampler_iterators.append(cur_sampler_iterator)\n",
    "            datasets_length.append(len(cur_dataset))\n",
    "        push_index_val = [0] + self.dataset.cumulative_sizes[:-1]\n",
    "        step = self.batch_size * self.number_of_datasets\n",
    "        samples_to_grab = self.batch_size\n",
    "        largest_dataset_index = torch.argmax(torch.as_tensor(datasets_length)).item()\n",
    "        # for this case we want to get all samples in dataset, this force us to resample from the smaller datasets\n",
    "        epoch_samples = datasets_length[largest_dataset_index] * self.number_of_datasets\n",
    "        final_samples_list = []  # this is a list of indexes from the combined dataset\n",
    "        for _ in range(0, epoch_samples, step):\n",
    "            for i in range(self.number_of_datasets):\n",
    "                cur_batch_sampler = sampler_iterators[i]\n",
    "                cur_samples = []\n",
    "                for _ in range(samples_to_grab):\n",
    "                    try:\n",
    "                        cur_sample_org = cur_batch_sampler.__next__()\n",
    "                        cur_sample = cur_sample_org + push_index_val[i]\n",
    "                        cur_samples.append(cur_sample)\n",
    "                    except StopIteration:\n",
    "                        if i == largest_dataset_index:\n",
    "                            # largest dataset iterator is done we can break\n",
    "                            samples_to_grab = len(cur_samples)  # adjusting the samples_to_grab\n",
    "                            # got to the end of iterator - extend final list and continue to next task if possible\n",
    "                            break\n",
    "                        else:\n",
    "                            # restart the iterator - we want more samples until finishing with the largest dataset\n",
    "                            sampler_iterators[i] = samplers_list[i].__iter__()\n",
    "                            cur_batch_sampler = sampler_iterators[i]\n",
    "                            cur_sample_org = cur_batch_sampler.__next__()\n",
    "                            cur_sample = cur_sample_org + push_index_val[i]\n",
    "                            cur_samples.append(cur_sample)\n",
    "                final_samples_list.extend(cur_samples)\n",
    "\n",
    "        return iter(final_samples_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter, test_iter = BucketIterator.splits((train_sets, dev_sets, test_sets),\n",
    "                                                        batch_sizes=(BSIZE, BSIZE*2, BSIZE*2),\n",
    "                                                        sort_within_batch=False,\n",
    "                                                        sort_key=lambda x: len(x.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConcatDataset' object has no attribute 'fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9919119fad82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/prod/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/prod/lib/python3.7/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# copy field names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             self.input_fields = [k for k, v in dataset.fields.items() if\n\u001b[1;32m     27\u001b[0m                                  v is not None and not v.is_target]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConcatDataset' object has no attribute 'fields'"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.label]:[torch.LongTensor of size 64]\n",
       "\t[.text]:('[torch.LongTensor of size 64x890]', '[torch.LongTensor of size 64]')\n",
       "\t[.task]:[torch.LongTensor of size 64]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 890]), torch.Size([64]), torch.Size([64]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text[0].shape, batch.task.shape, batch.label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1321\n",
       "9     1319\n",
       "12    1318\n",
       "5     1286\n",
       "2     1283\n",
       "3     1276\n",
       "7     1271\n",
       "6     1271\n",
       "8     1269\n",
       "13    1258\n",
       "4     1258\n",
       "11    1242\n",
       "1     1198\n",
       "10    1174\n",
       "Name: task_id, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.task_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8981\n",
       "0    8763\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prod] *",
   "language": "python",
   "name": "conda-env-prod-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
