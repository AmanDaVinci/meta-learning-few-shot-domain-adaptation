{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Eval\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from meta_infomax.datasets.fudan_reviews import prepare_data, get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSIZE = 16\n",
    "ENCODER_DIM = 100\n",
    "CLASSIFIER_DIM = 100\n",
    "NUM_TASKS = 14\n",
    "EPOCHS = 1\n",
    "DATASETS = ['apparel', 'baby', 'books', 'camera_photo',  'electronics', \n",
    "      'health_personal_care', 'imdb', 'kitchen_housewares', 'magazines', \n",
    "      'music', 'software', 'sports_outdoors', 'toys_games', 'video']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data()\n",
    "train_set, dev_set, test_set, vocab = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, dev_iter, test_iter = BucketIterator.splits((train_set, dev_set, test_set),\n",
    "                                                        batch_sizes=(BSIZE, BSIZE*2, BSIZE*2),\n",
    "                                                        sort_within_batch=False,\n",
    "                                                        sort_key=lambda x: len(x.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 16]\n",
       "\t[.label]:[torch.LongTensor of size 16]\n",
       "\t[.text]:('[torch.LongTensor of size 16x707]', '[torch.LongTensor of size 16]')\n",
       "\t[.task]:[torch.LongTensor of size 16]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 707]), torch.Size([16]), torch.Size([16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text[0].shape, batch.label.shape, batch.task.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,emb_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.h0 = self.h0.to(x.device)\n",
    "        self.c0 = self.c0.to(x.device)\n",
    "        out, _ = self.lstm(x, (self.h0, self.c0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskInfoMax(nn.Module):\n",
    "    \n",
    "    def __init__(self, shared_encoder, embeddings, vocab, encoder_dim, encoder_layers, classifier_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(embeddings, freeze=True, padding_idx=vocab.stoi[\"<pad>\"])\n",
    "        self.shared_encoder = shared_encoder\n",
    "        self.private_encoder = Encoder(embeddings.shape[-1], encoder_dim, encoder_layers)\n",
    "        self.classifier = Classifier(encoder_dim*4, classifier_dim, out_dim)\n",
    "    \n",
    "    def forward(self, sentences, lengths):\n",
    "        sent_embed = self.emb(sentences)\n",
    "        shared_out = self.shared_encoder(sent_embed)\n",
    "        private_out = self.private_encoder(sent_embed)\n",
    "        h = torch.cat((shared_out, private_out), dim=1)\n",
    "        out = self.classifier(h)\n",
    "        return out, shared_out, private_out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65551, 300])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (lstm): LSTM(300, 100, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_encoder = Encoder(vocab.vectors.shape[1], ENCODER_DIM, 1)\n",
    "shared_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_models = [MultiTaskInfoMax(shared_encoder=shared_encoder, embeddings=vocab.vectors, vocab=vocab, \n",
    "                                     encoder_dim=ENCODER_DIM,encoder_layers=1, classifier_dim=CLASSIFIER_DIM, out_dim=2)\n",
    "                    for i in range(len(DATASETS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskInfoMax(\n",
       "  (emb): Embedding(65551, 300, padding_idx=1)\n",
       "  (shared_encoder): Encoder(\n",
       "    (lstm): LSTM(300, 100, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (private_encoder): Encoder(\n",
       "    (lstm): LSTM(300, 100, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=400, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multitask_models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_models[batch]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:infomax]",
   "language": "python",
   "name": "conda-env-infomax-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
